{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "raw",
      "source": "1. What does R-squared represent in a regression model?\nAnswer.1 - R-squared measures the proportion of variance in the dependent variable that can be explained by the independent variables in the model. It ranges from 0 to 1.\n\n2. What are the assumptions of linear regression?\nAnswer2 - The key assumptions are:\nLinearity\nIndependence of errors\nHomoscedasticity (constant variance of errors)\nNormality of errors\nNo multicollinearity among independent variables\n\n3. What is the difference between R-squared and Adjusted R-squared?\nAnswer.3 R-squared increases with the addition of predictors, regardless of their relevance. Adjusted R-squared adjusts for the number of predictors and only increases if the new predictor improves the model.\n\n4. Why do we use Mean Squared Error (MSE)?\nAnswer 4. MSE is used to measure the average of the squares of the errors, providing a way to quantify the accuracy of a regression model.\n\n\n5. What does an Adjusted R-squared value of 0.85 indicate?\nAnswer.5 It indicates that 85% of the variance in the dependent variable is explained by the model, adjusted for the number of predictors used.\n\n6. How do we check for normality of residuals in linear regression?\nAnswer.6 Use graphical methods like Q-Q plots or statistical tests like the Shapiro-Wilk test.\n\n7. What is multicollinearity, and how does it impact regression?\nAnswer.7 Multicollinearity occurs when independent variables are highly correlated, leading to unreliable coefficient estimates.\n\n\n8. What is Mean Absolute Error (MAE)?\nAnswer.8 MAE is the average of the absolute differences between predicted and actual values, providing a measure of error magnitude.\n\n\n9. What are the benefits of using an ML pipeline?\nAnswer.9 ML pipelines streamline the workflow by chaining preprocessing steps and modeling, improving reproducibility and efficiency.\n\n10. Why is RMSE considered more interpretable than MSE?\nAnswer.10 RMSE is in the same units as the target variable, making it easier to interpret compared to MSE, which is in squared units.\n\n11. What is pickling in Python, and how is it useful in ML?\nAnswer.11 Pickling serializes Python objects into a byte stream, allowing for easy saving and loading of models and data.\n\n12. What does a high R-squared value mean?\nAnswer.12 A high R-squared indicates a good fit, where the model explains a large proportion of the variance in the dependent variable.\n\n13. What happens if linear regression assumptions are violated?\nAnswer.13 Violations can lead to biased, inefficient, or inconsistent estimates and poor model performance.\n\n14. How can we address multicollinearity in regression?\nAnswer.14 By removing highly correlated predictors, using regularization techniques (like Lasso or Ridge regression), or performing dimensionality reduction (like PCA).\n\n\n15. Why do we use pipelines in machine learning?\nAnswer.15 Pipelines ensure consistent application of preprocessing and modeling steps, making the workflow more efficient and less error-prone.\n\n\n16. How is Adjusted R-squared calculated?\nAnswer.16 Adjusted R-squared = 1 - [(1-RÂ²)(n-1)/(n-p-1)], where n is the number of observations and p is the number of predictors.\n\n17. Why is MSE sensitive to outliers?\nAnswer.17  MSE squares the errors, giving disproportionate weight to larger errors, which makes it sensitive to outliers.\n\n18. What is the role of homoscedasticity in linear regression?\nAnswer.18 Homoscedasticity ensures that the variance of the residuals is constant across all levels of the independent variables, leading to more reliable standard errors and confidence intervals.\n\n19. What is Root Mean Squared Error (RMSE)?\nAnswer.19 RMSE is the square root of the MSE, providing a measure of error in the same units as the target variable.\n\n\n20. Why is pickling considered risky?\nAnswer.20 Pickling can execute arbitrary code during deserialization, leading to potential security risks if the pickle data is from an untrusted source.\n\n\n21. What alternatives exist to pickling for saving ML models?\nAnswer.21 Alternatives include Joblib, ONNX, PMML, and custom serialization formats like JSON or HDF5.\n\n22. What is heteroscedasticity, and why is it a problem?\nAnswer.22 Heteroscedasticity is when the variance of residuals is not constant. It can lead to inefficient and biased estimates in regression.\n\n23. How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\nAnswer.23 R-squared will increase or stay the same, but Adjusted R-squared may decrease if the new predictors do not improve the model significantly.",
      "metadata": {}
    }
  ]
}